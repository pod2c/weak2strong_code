{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e1c000-38a6-4dc0-91dc-a320807d2443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers accelerate datasets bitsandbytes einops wandb trl peft scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5208ba-fb3e-484c-9d22-3efdca83a035",
   "metadata": {},
   "source": [
    "# 分别导入sciq的训练集和测试集，并且将训练集和测试集都转换成二分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9208fa4-0cc4-4d0f-b904-c317452eefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载sciq数据集的训练集和测试集\n",
    "train_dataset = load_dataset(\"allenai/sciq\", split=\"train\")\n",
    "validation_dataset = load_dataset(\"allenai/sciq\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"allenai/sciq\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e01c8-17c0-473a-8d3a-36071ed30ca4",
   "metadata": {},
   "source": [
    "# 合并训练集与验证集，组成新的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d335a020-e4d9-417b-aede-c7078f013b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# 合并训练集和验证集\n",
    "train_data = concatenate_datasets([train_dataset, validation_dataset])\n",
    "train_data = train_data.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc31b6e-3324-46f2-b3ff-bea48504d8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which group of metals in the periodic table include elements such as sodium and potassium?',\n",
       " 'distractor3': 'lanthanides',\n",
       " 'distractor1': 'igneous metals',\n",
       " 'distractor2': 'actinides',\n",
       " 'correct_answer': 'alkali metals',\n",
       " 'support': 'The first step in the process of inductive reasoning is making specific observations. In the periodic table of elements, which we will discuss later, there is a group of metals with similar properties called the alkali metals. The alkali metals include elements such as sodium and potassium. If I put sodium or potassium in water, I will observe a very violent reaction every time. I draw a general conclusion from these observations: all alkali metals will react violently with water.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19667fe8-e29e-4b5b-b5b2-a426dcf6ff20",
   "metadata": {},
   "source": [
    "# 将数据集转化为一个二分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f35990-02e4-4e61-88cc-6554ba65a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset,Features, Value\n",
    "\n",
    "# 定义转换为二元分类数据集的函数\n",
    "def process_correct_answer(example):\n",
    "    return {\n",
    "        'question': example['question'],\n",
    "        'answer_option': example['correct_answer'],\n",
    "        'label': 1\n",
    "    }\n",
    "\n",
    "def process_distractor(example, distractor_key):\n",
    "    return {\n",
    "        'question': example['question'],\n",
    "        'answer_option': example[distractor_key],\n",
    "        'label': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff35db-eb1c-47cc-a0bf-b6e83574b01a",
   "metadata": {},
   "source": [
    "# 预处理训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2dd54c-77a3-42fc-81a7-936d6e1431db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facca5f5026c4d2e9d27e5709665620c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba871fa54c3e4e278f6ac44f4332dcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca8b5d7f33246bda2655271540ea933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93b435d81a4444f85bdcf332bd87697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 对训练集进行转换\n",
    "train_correct_answers = train_data.map(process_correct_answer)\n",
    "train_distractor1 = train_data.map(process_distractor, fn_kwargs={'distractor_key': 'distractor1'})\n",
    "train_distractor2 = train_data.map(process_distractor, fn_kwargs={'distractor_key': 'distractor2'})\n",
    "train_distractor3 = train_data.map(process_distractor, fn_kwargs={'distractor_key': 'distractor3'})\n",
    "train_dataset = concatenate_datasets([train_correct_answers, train_distractor1, train_distractor2, train_distractor3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27139992-3cde-4759-965d-734103c8553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae68f4c-fecf-4a6f-8168-e0c87a338395",
   "metadata": {},
   "source": [
    "# 将原始数据集一分为二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826c6495-5b89-431d-b3fc-87702f3bc8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'],\n",
       "    num_rows: 50716\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0aa1b7a-c87a-46bd-8a01-569ac3ede99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_subset(dataset, n_docs, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    if n_docs is None or n_docs > len(dataset):\n",
    "        return dataset\n",
    "    else:\n",
    "        indices = random.sample(range(len(dataset)), n_docs)\n",
    "        return dataset.select(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba9afe0-8a07-4a70-9e94-b280dccff311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weak_model = create_subset(train_dataset, 20000, seed=53)\n",
    "train_weak_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6ceb63-4e1c-42bf-a852-59851f5aa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split_data = train_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "#train_ds1, train_ds2 = split_data['train'], split_data['test']\n",
    "\n",
    "#print('len(train_weak_model):', len(train_ds1), 'len(label_weak_model):', len(train_ds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba5889-6f67-4a44-a008-189bd353de59",
   "metadata": {},
   "source": [
    "# 先处理用来训练的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b72bf4-529b-437f-a2ae-e33c465700d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weak_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e5754cf-1f9f-4d85-a7e5-d6a9d865e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将txt和label字段合并为一个整体\n",
    "def merge_fields(example):\n",
    "    text = example['question']\n",
    "    distractor1 = example['distractor1']\n",
    "    distractor2 = example['distractor2']\n",
    "    distractor3 = example['distractor3']\n",
    "    correct_answer = example['correct_answer']\n",
    "    support = example['support']\n",
    "    answer_option = example['answer_option']\n",
    "    label = example['label']\n",
    "    merged_input = f\"Text: {text}\\nDistractor1: {distractor1}\\nDistractor2: {distractor2}\\nDistractor1: {distractor3}\\ncorrect_answer: {correct_answer}\\nsupport: {support}\\nanswer_option: {answer_option}\\nLabel: {label}\"\n",
    "    return {'merged_input': merged_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eb9ae0e-c763-4684-995e-675dfcc02cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdea37f93ad4f3b843f748c86495cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_weak_model = train_weak_model.map(merge_fields, remove_columns=['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40a99108-5f96-4d2d-a798-65c054f0d956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'merged_input': 'Text: The theory of evolution by what (and other processes) explains both the diversity of organisms and how populations of organisms change over time?\\nDistractor1: natural evolution\\nDistractor2: characteristic selection\\nDistractor1: genocide\\ncorrect_answer: natural selection\\nsupport: Biology has only a few over arching theories. One of these, the Cell Theory of Life, explains the historic continuity of organisms, while the Theory of Evolution by Natural Selection (and other processes), explains both the diversity of organisms and how populations of organisms change over time. Finally, the Physicochemical Theory of Life explains how it is that organisms can display their remarkable properties without violating the laws that govern all physical and chemical systems.40 What is life, exactly? Clearly, if we are going to talk about biology, and organisms and cells and such, we have to define exactly what we mean by life. This raises a problem peculiar to biology as a science. We cannot define life generically because we know of only one type of life. We do not know whether this type of life is the only type of life possible or whether radically different forms of life exist elsewhere in the universe or even on Earth, in as yet to be recognized forms. While you might think that we know of many different types of life, from mushrooms to whales, from humans to the bacterial communities growing on the surfaces of our teeth (that is what dental plaque is, after all), we will discover that the closer we look the more these different “types of life” are in fact all versions of a common underlying motif, they represent versions of a single type of life. Based on their common chemistry, molecular composition, cellular structure, and the way that they encode, read, and use hereditary information in the form of molecules of deoxyribonucleic acid (DNA), all topics we will consider in depth later on, there is no reasonable doubt that all organisms are related, they are descended from a common ancestor. We cannot currently answer the question of whether the origin of life is a simple, likely, and predictable event given the conditions that existed on the Earth when life first arose, or whether it is an extremely rare and unlikely event. In the absence of empirical data, one can question whether scientists are acting scientifically or more as lobbyists for their own pet projects when they talk about doing astrobiology or speculating on when and where we will discover alien life forms. That said, asking seemingly silly questions, provided that empirically-based answers can be generated, has often been the critical driver of scientific progress. Consider, for example, current searches for life on Earth, almost all of which are based on what we already know about life. Specifically, most of the methods used rely on the fact that all known organisms use DNA to encode their genetic information; these methods would not be expected to recognize dramatically different types of life; they certainly would not detect organisms that used a non-DNA method to encode genetic information. If we could generate living systems de novo in the laboratory we would have a better understanding of what functions are necessary for life and how to look for possible “non-standard” organisms using better methods. It might even lead to the discovery of alternative forms of life right here on Earth, assuming they exist.41 That said, until someone manages to create or identify such non-standard forms of life, it seems quite reasonable to concentrate on the characteristics of life as we know them.\\nanswer_option: natural evolution\\nLabel: 0'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weak_model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24753637-a2a8-4645-b98c-78fd59f3cf95",
   "metadata": {},
   "source": [
    "# 导入弱模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "064e9385-96f3-48e4-9dcb-369df720d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = 'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    #num_labels=2\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5771e1-5811-49ae-b20b-165e919eabeb",
   "metadata": {},
   "source": [
    "# 设置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5321cf5a-e5e3-4cc5-9d3e-f5d8c9d61289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = './results'\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'paged_adamw_32bit'\n",
    "save_steps = 1000\n",
    "logging_steps = 10\n",
    "learning_rate = 5e-5\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 1000\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = 'constant'\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecf953-adcf-4f3a-b80d-8144b46cb373",
   "metadata": {},
   "source": [
    "# 设置训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aac7a9f-2b75-4d41-a4e3-6deb2310ef12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4a52760fc64773a694c21686c6036d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_weak_model,\n",
    "    dataset_text_field='merged_input',\n",
    "    #label_field='label',\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if 'norm' in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d59e82-d667-47d3-aaf0-0ca5d29cb557",
   "metadata": {},
   "source": [
    "# 弱模型训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e41be5-6a62-45c5-8a7b-0f3493bb2d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms1820587\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240516_123753-2qduucsp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/s1820587/huggingface/runs/2qduucsp' target=\"_blank\">rose-blaze-194</a></strong> to <a href='https://wandb.ai/s1820587/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/s1820587/huggingface' target=\"_blank\">https://wandb.ai/s1820587/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/s1820587/huggingface/runs/2qduucsp' target=\"_blank\">https://wandb.ai/s1820587/huggingface/runs/2qduucsp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 100/3750 00:20 < 12:31, 4.86 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.820100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3138\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3138\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3141\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3161\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3160\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3161\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3163\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1338\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1338\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1341\u001b[0m     output \u001b[38;5;241m=\u001b[39m (lm_logits,) \u001b[38;5;241m+\u001b[39m transformer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "401cd78f-9abc-47bd-87a1-ad1420c3aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "model_to_save.save_pretrained('weak_model_outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfc4f7-aad6-4558-9b0a-9ff030cabd8d",
   "metadata": {},
   "source": [
    "# 现在来处理用于标记的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b185209-9c61-4fc1-8868-06dceca4d4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'],\n",
       "    num_rows: 30716\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_weak_model = create_subset(train_dataset, 30716, seed=26)\n",
    "label_weak_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49cb8bcc-cba0-46b9-9b0f-dbd81e71b823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd6aa224cb94eab83d49fe535840fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/31 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20826443"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_weak_model.to_json(\"label_weak_model.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "093b856a-8f76-416d-909d-30dbe38a815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option'],\n",
      "    num_rows: 30716\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# 假设你的原始数据集名为 \"dataset\"\n",
    "label_dataset = label_weak_model.remove_columns(['label'])\n",
    "\n",
    "# 查看更新后的数据集结构\n",
    "print(label_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d99e6a-d6b0-4661-a000-d74bb53dcbf7",
   "metadata": {},
   "source": [
    "# 给数据集打标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0255f81b-92f1-48d4-a7cb-aff1d19d1b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d134d59b60249a0b5d35e8a74e30f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling dataset: 100%|██████████| 30716/30716 [00:01<00:00, 18031.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'predicted_label'],\n",
      "    num_rows: 30716\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c6dcfa3e74416fb1bbdff5000be133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/31 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "21138061"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# 设置日志级别为ERROR,以抑制警告信息\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# 加载微调后的tokenizer和模型\n",
    "model_path = \"weak_model_outputs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')\n",
    "\n",
    "# 将模型移动到GPU(如果可用)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuned_model.to(device)\n",
    "\n",
    "# 定义分类标签\n",
    "label_map = {0: \" No\", 1: \" Yes\"}\n",
    "\n",
    "# 对数据集进行预处理和打标签\n",
    "def preprocess_and_label(example):\n",
    "    # 将问题、答案选项和其他相关信息拼接成一个完整的输入文本\n",
    "    input_text = f\"Question: {example['question']}\\n\"\n",
    "    input_text += f\"Answer Option: {example['answer_option']}\\n\"\n",
    "    input_text += f\"Correct Answer: {example['correct_answer']}\\n\"\n",
    "    input_text += f\"Distractor 1: {example['distractor1']}\\n\"\n",
    "    input_text += f\"Distractor 2: {example['distractor2']}\\n\"\n",
    "    input_text += f\"Distractor 3: {example['distractor3']}\\n\"\n",
    "    input_text += f\"Support: {example['support']}\\n\"\n",
    "    input_text += \"Label:\"\n",
    "    \n",
    "    # 对输入文本进行预处理\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # 将输入数据移动到与模型相同的设备\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # 使用模型进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = finetuned_model.generate(input_ids, max_new_tokens=10, num_return_sequences=1)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 判断生成的文本属于哪个类别\n",
    "    if label_map[0] in generated_text:\n",
    "        predicted_label = 0\n",
    "    elif label_map[1] in generated_text:\n",
    "        predicted_label = 1\n",
    "    else:\n",
    "        predicted_label = -1 # 如果生成的文本不包含任何一个标签,则认为分类失败\n",
    "    \n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"answer_option\": example[\"answer_option\"],\n",
    "        \"correct_answer\": example[\"correct_answer\"],\n",
    "        \"distractor1\": example[\"distractor1\"],\n",
    "        \"distractor2\": example[\"distractor2\"],\n",
    "        \"distractor3\": example[\"distractor3\"],\n",
    "        \"support\": example[\"support\"],\n",
    "        \"predicted_label\": predicted_label\n",
    "    }\n",
    "\n",
    "# 对数据集进行处理和打标签\n",
    "labeled_dataset = label_dataset.map(preprocess_and_label)\n",
    "\n",
    "# 使用tqdm显示进度条\n",
    "with tqdm(total=len(label_dataset), desc=\"Labeling dataset\") as pbar:\n",
    "    for example in labeled_dataset:\n",
    "        pbar.update(1)\n",
    "\n",
    "# 查看打标签后的数据集\n",
    "print(labeled_dataset)\n",
    "\n",
    "labeled_dataset.to_json(\"labeled_dataset.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff3e58-48e3-4297-97c4-4d51725769bb",
   "metadata": {},
   "source": [
    "# 将带弱标签的数据与真实标签的数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d3f875-6490-43e8-9c09-0fc8571f4f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'],\n",
       "    num_rows: 30716\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_weak_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21704cfa-bd50-4c0f-b6ab-82998d516bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'],\n",
       "    num_rows: 30716\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b460389-e6d8-4667-b44d-a853f7be16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取JSON Lines数据到pandas DataFrame\n",
    "df = pd.read_json(\"labeled_dataset.jsonl\", lines=True)\n",
    "\n",
    "# 重命名 predicted_label 列为 label\n",
    "df = df.rename(columns={\"predicted_label\": \"label\"})\n",
    "\n",
    "# 将修改后的DataFrame写回JSON Lines文件\n",
    "df.to_json(\"labeled_dataset.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fb9e929-ab70-4539-babe-edb81022234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "merged_dataset = concatenate_datasets([labeled_dataset, label_weak_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d845801-05e6-486f-8dca-262774f8ff4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'],\n",
       "    num_rows: 61432\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9223928-6d5f-4dfa-bc6d-659b08a65f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergerd_dataset = merged_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fe360ba-9f0d-4861-a114-bfa03d73840c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What incredibly successful species has quickly colonized almost all of earth’s terrestrial habitats, but also impacted earth, its climate, and its environment?',\n",
       " 'distractor3': 'fish',\n",
       " 'distractor1': 'chimpanzees',\n",
       " 'distractor2': 'birds',\n",
       " 'correct_answer': 'humans',\n",
       " 'support': 'The human species has been incredibly successful. In a relatively short period of time, it has colonized almost all of Earth’s terrestrial habitats. Unfortunately, human beings have also impacted Earth, its climate, and its environment. Human actions threaten Earth’s valuable biodiversity.',\n",
       " 'answer_option': 'chimpanzees',\n",
       " 'label': -1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63f41a1b-0f88-46ae-afac-887c56e161eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c805248d7c8d4280a33efbe4f9f8b4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "41582885"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.to_json(\"merged_dataset.json\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
