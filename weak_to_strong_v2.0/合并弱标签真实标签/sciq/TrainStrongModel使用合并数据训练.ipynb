{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53446a9-2ae4-415d-8293-c50d7119e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers accelerate datasets bitsandbytes einops wandb trl peft scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c8a421-e84c-4bb1-9b6e-48b58d95abc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6ac3c6323645e59921f20b35d5e91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"merged_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c54c35d-6a96-4906-9eb1-33a4236dc713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'],\n",
       "    num_rows: 61432\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c54472-3408-4af8-a058-c8e95ccba928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将txt和label字段合并为一个整体\n",
    "def merge_fields(example):\n",
    "    text = example['question']\n",
    "    distractor1 = example['distractor1']\n",
    "    distractor2 = example['distractor2']\n",
    "    distractor3 = example['distractor3']\n",
    "    correct_answer = example['correct_answer']\n",
    "    support = example['support']\n",
    "    answer_option = example['answer_option']\n",
    "    label = example['label']\n",
    "    merged_input = f\"Text: {text}\\nDistractor1: {distractor1}\\nDistractor2: {distractor2}\\nDistractor1: {distractor3}\\ncorrect_answer: {correct_answer}\\nsupport: {support}\\nanswer_option: {answer_option}\\nLabel: {label}\"\n",
    "    return {'merged_input': merged_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c9ef33-d3ad-4d24-8483-4cd24cfaf6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9952874851a7464f965c9fef7f03e9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61432 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(merge_fields, remove_columns=['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'answer_option', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f808ae-b207-4550-bc66-50d805ffed2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'merged_input': 'Text: \\n\\nHuman: Can you teach me about the chakras of the body?\\n\\nAssistant: Sure.  There are many different spiritual traditions that recognize different centers of energy and power along the spine, from the base of the spine to the crown.  We could study this in a variety of ways.  I could teach you about how the chakras are viewed by different traditions; maybe you’d be interested in looking up information for yourself, and I can help you interpret what you find.  It might be easier for you to understand if we start with one chakra and work our way up to the crown.  What do you think?\\n\\nHuman: Are chakras something that are scientifically proven?\\n\\nAssistant: It depends on what you mean by “scientifically proven.”  In Western science, it would be hard to study the functioning of the chakras because they can’t be quantified by traditional scientific methods, but that doesn’t mean they don’t exist.  \\nSome scholars view the chakras as a metaphor to describe certain states of consciousness, while others view them as channels through which energy flows into the body.  Other traditions view the chakras as aspects of the natural world.  According to some sources, the chakras are also the intersection of different energy bodies, rather than individual channels.  \\nA number of studies have been done looking at the differences between people’s self-reported chakra activation and their ability to perform certain tasks.  In the positive studies, the most consistent finding is that the chakra corresponding to the brain (the crown chakra) appears to have the most influence on performance of verbal, non-spatial tasks.\\n\\nHuman: What are the different chakras?\\n\\nAssistant: The seven main chakras are the root, the sacral, the solar plexus, the heart, the throat, the brow, and the crown.  While many different cultures have traditions surrounding chakras, the only one that’s specifically mentioned in the Bible is the brow chakra, which is sometimes called the “third eye” chakra.\\nLabel: -1'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87db65ad-0f54-4b9f-9916-e9ded838c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = 'gpt2-medium'\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(\n",
    "    #load_in_4bit=True,\n",
    "    #bnb_4bit_quant_type='nf4',\n",
    "    #bnb_4bit_compute_dtype=torch.float16,\n",
    "#)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    #quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d0927b-421b-4d54-b06a-891f964e3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7377d9c7-6e0c-4831-a8c2-4c0519e3482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = './results3'\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'paged_adamw_32bit'\n",
    "save_steps = 1000\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-5\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 1000\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = 'constant'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    #fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5d3a2ff-4e9a-4300-aadd-e470596ef9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076eae30b9784c72b1790daf1a8e8f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "#from accelerate import Accelerator\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "#accelerator = Accelerator()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    #peft_config=peft_config,\n",
    "    dataset_text_field='merged_input',\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if 'norm' in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23fa6b5b-fe25-4db4-ac30-8611b87610bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2018, 'grad_norm': 1.0542086362838745, 'learning_rate': 2e-05, 'epoch': 0.004}\n",
      "{'loss': 2.0516, 'grad_norm': 1.1375232934951782, 'learning_rate': 2e-05, 'epoch': 0.008}\n",
      "{'loss': 2.0102, 'grad_norm': 1.34402596950531, 'learning_rate': 2e-05, 'epoch': 0.012}\n",
      "{'loss': 1.9327, 'grad_norm': 1.632599115371704, 'learning_rate': 2e-05, 'epoch': 0.016}\n",
      "{'loss': 1.7099, 'grad_norm': 2.380119800567627, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 2.1061, 'grad_norm': 1.0003641843795776, 'learning_rate': 2e-05, 'epoch': 0.024}\n",
      "{'loss': 1.9977, 'grad_norm': 1.063649296760559, 'learning_rate': 2e-05, 'epoch': 0.028}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f60180-5f6c-4936-abeb-582f79975406",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "model_to_save.save_pretrained('gpt2medium_strong_model_outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d14487-b82c-4d42-93c3-5746f5c67522",
   "metadata": {},
   "source": [
    "# 使用测试集对微调后的模型进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827f49d-0408-4d1d-bf65-9e46f077723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import logging\n",
    "\n",
    "# 设置日志级别为ERROR, 以抑制警告信息\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# 加载微调后的tokenizer和模型\n",
    "model_path = \"gpt2medium_strong_model_outputs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuned_model.to(device)\n",
    "\n",
    "# 定义分类标签\n",
    "label_map = {0: \" No\", 1: \" Yes\"}\n",
    "\n",
    "# 加载测试数据集\n",
    "#test_ds = load_dataset(\"json\", data_files=\"test_ds.jsonl\", split='train')\n",
    "\n",
    "# 对数据集进行预处理和打标签\n",
    "def preprocess_and_label(example):\n",
    "    # 将所有特征拼接为一个输入文本\n",
    "    input_text = f\"Question: {example['question']} Distractor3: {example['distractor3']} Distractor1: {example['distractor1']} Distractor2: {example['distractor2']} Correct Answer: {example['correct_answer']} Support: {example['support']} Answer Option: {example['answer_option']}\"\n",
    "    \n",
    "    # 对输入文本进行预处理\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # 将输入数据移动到与模型相同的设备\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = finetuned_model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=10, num_return_sequences=1)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 判断生成的文本属于哪个类别\n",
    "    if label_map[0] in generated_text:\n",
    "        predicted_label = 0\n",
    "    elif label_map[1] in generated_text:\n",
    "        predicted_label = 1\n",
    "    else:\n",
    "        predicted_label = None  # 如果生成的文本不包含任何一个标签，则不考虑这个样本\n",
    "\n",
    "    return {\"text\": input_text, \"label\": example[\"label\"], \"predicted_label\": predicted_label}\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = []\n",
    "labels = []\n",
    "total_examples = len(test_ds)\n",
    "print(f\"Total examples: {total_examples}\")\n",
    "\n",
    "progress_bar = tqdm(test_ds, desc=\"Evaluating\")\n",
    "for example in progress_bar:\n",
    "    processed_example = preprocess_and_label(example)\n",
    "    if processed_example[\"predicted_label\"] is not None:\n",
    "        # 只考虑有效的预测\n",
    "        predictions.append(processed_example[\"predicted_label\"])\n",
    "        labels.append(processed_example[\"label\"])\n",
    "\n",
    "# 打印最终的评估指标\n",
    "print(f\"Accuracy: {accuracy_score(labels, predictions):.4f}\")\n",
    "print(f\"F1 score: {f1_score(labels, predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a6e69-5c2b-4be2-80d7-6f1af841f657",
   "metadata": {},
   "source": [
    "# 使用测试集对原始模型进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d331521-92d3-429e-bf13-3ffe98a50a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import logging\n",
    "\n",
    "# 设置日志级别为ERROR, 以抑制警告信息\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# 加载微调后的tokenizer和模型\n",
    "model_path = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuned_model.to(device)\n",
    "\n",
    "# 定义分类标签\n",
    "label_map = {0: \" No\", 1: \" Yes\"}\n",
    "\n",
    "# 加载测试数据集\n",
    "#test_ds = load_dataset(\"json\", data_files=\"test_ds.jsonl\", split='train')\n",
    "\n",
    "# 对数据集进行预处理和打标签\n",
    "def preprocess_and_label(example):\n",
    "    # 将所有特征拼接为一个输入文本\n",
    "    input_text = f\"Question: {example['question']} Distractor3: {example['distractor3']} Distractor1: {example['distractor1']} Distractor2: {example['distractor2']} Correct Answer: {example['correct_answer']} Support: {example['support']} Answer Option: {example['answer_option']}\"\n",
    "    \n",
    "    # 对输入文本进行预处理\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # 将输入数据移动到与模型相同的设备\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = finetuned_model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=10, num_return_sequences=1)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 判断生成的文本属于哪个类别\n",
    "    if label_map[0] in generated_text:\n",
    "        predicted_label = 0\n",
    "    elif label_map[1] in generated_text:\n",
    "        predicted_label = 1\n",
    "    else:\n",
    "        predicted_label = None  # 如果生成的文本不包含任何一个标签，则不考虑这个样本\n",
    "\n",
    "    return {\"text\": input_text, \"label\": example[\"label\"], \"predicted_label\": predicted_label}\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = []\n",
    "labels = []\n",
    "total_examples = len(test_ds)\n",
    "print(f\"Total examples: {total_examples}\")\n",
    "\n",
    "progress_bar = tqdm(test_ds, desc=\"Evaluating\")\n",
    "for example in progress_bar:\n",
    "    processed_example = preprocess_and_label(example)\n",
    "    if processed_example[\"predicted_label\"] is not None:\n",
    "        # 只考虑有效的预测\n",
    "        predictions.append(processed_example[\"predicted_label\"])\n",
    "        labels.append(processed_example[\"label\"])\n",
    "\n",
    "# 打印最终的评估指标\n",
    "print(f\"Accuracy: {accuracy_score(labels, predictions):.4f}\")\n",
    "print(f\"F1 score: {f1_score(labels, predictions):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
